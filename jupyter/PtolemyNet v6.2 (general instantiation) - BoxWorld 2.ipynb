{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c923be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 10:36:11.474315: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-04 10:36:11.483356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743788171.494178   53189 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743788171.497759   53189 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743788171.506707   53189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743788171.506724   53189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743788171.506725   53189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743788171.506726   53189 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-04 10:36:11.509481: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_keras.layers import *\n",
    "import tf_keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import scipy\n",
    "\n",
    "%load_ext tensorboard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d880b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enable CUDA unified memory\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 4\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc0213",
   "metadata": {},
   "source": [
    "# Image loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560a5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_size = 50\n",
    "y_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d90e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_vecs(img):\n",
    "    output = np.zeros((x_size * y_size, 1, 2))\n",
    "    rescaled_img = 2.0 * img.flatten() - 1.0\n",
    "    output[:, 0, 0] = np.sin(rescaled_img * np.pi / 2.0)\n",
    "    output[:, 0, 1] = np.cos(rescaled_img * np.pi / 2.0)\n",
    "    return output\n",
    "\n",
    "def vecs_to_img(vecs):\n",
    "    v = vecs.reshape((x_size, y_size, 1, 2))\n",
    "    v = v / np.linalg.norm(v, axis=-1, keepdims=True)\n",
    "    img = np.arcsin(v[:, :, 0, 0:1]) / np.pi * 2.0\n",
    "    img = (1.0 + img) / 2.0\n",
    "    img = 255.0 * np.clip(img, 0.0, 1.0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e23e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 200\n",
    "num_frames = 60\n",
    "num_t = 3\n",
    "\n",
    "num_inputs_per_run = num_frames // num_t\n",
    "num_inputs = num_runs * num_inputs_per_run\n",
    "X = np.zeros((num_inputs, num_t, x_size * y_size, 1, 2))\n",
    "for run_idx in range(num_runs):\n",
    "    for frame_idx in range(0, num_frames, num_t):\n",
    "        i = run_idx * num_inputs_per_run + frame_idx // num_t\n",
    "        for t in range(num_t):\n",
    "            img_path = f\"./box_world_1/{str(run_idx).zfill(4)}-{str(frame_idx + t).zfill(4)}.png\"\n",
    "            img = keras.utils.load_img(img_path, color_mode=\"grayscale\", target_size=(x_size, y_size))\n",
    "            img_arr = keras.utils.img_to_array(img) / 255.0\n",
    "            X[i][t] = img_to_vecs(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e37b8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.zeros((num_inputs, num_t, x_size * y_size, 1, 2))\n",
    "for run_idx in range(num_runs):\n",
    "    for frame_idx in range(0, num_frames, num_t):\n",
    "        i = run_idx * num_inputs_per_run + frame_idx // num_t\n",
    "        for t in range(num_t):\n",
    "            img_path = f\"./box_world_2/{str(run_idx).zfill(4)}-{str(frame_idx + t).zfill(4)}.png\"\n",
    "            img = keras.utils.load_img(img_path, color_mode=\"grayscale\", target_size=(x_size, y_size))\n",
    "            img_arr = keras.utils.img_to_array(img) / 255.0\n",
    "            X2[i][t] = img_to_vecs(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c127f4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAyADIBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiitjwnY2+p+MtDsLyPzLW61C3hmTcRuRpFDDI5GQT0r2mT4e6BFI0cnhjREdCVZW1eQEEdQR9pqjP4T8MW1xLbzeE7FZYnKOv2q6OCDgjiauA+JOkaZpGqaUNKsI7KK508TSRRySOu/zpUyC7MeiL3ri6KK6DwJ/yUPw1/2FbX/0ate96tps1zrN9cQz2LRS3Ejo326EZBYkHl6o61Ikuu6hJG6uj3MjKynIILHBBrzL4sf8hPQf+wV/7c3Fef0UVqeGtSh0bxVpGqXCyNBZXsNxIsYBYqjhiBkgZwPUV6X/AMLB8H/89tc/8F8P/wAfo/4WD4P/AOe2uf8Agvh/+P1xnj7xFpviPUtOl0sXfkWtkLdmuoljZm82R8gKzDGJAOvY1ydFFFFFFFFFFFFFFFFFFFFFFFf/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAAAZElEQVR4Ae2TwRIAEQxDy/hwf74OKznomi5H4tLSxsQzNdO6l0DqT38+EKA2lAr2FQmjP3lLmR2/kw2JNzbzQxeUWG8NFbZh7HKJg0z804SS+D9wx0mQMXkLUwkMiiIgAiIQEGikOAYvp6FYnwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = vecs_to_img(X2[101][0])\n",
    "keras.utils.array_to_img(img, scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18292c",
   "metadata": {},
   "source": [
    "# Layer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a599e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_norm(t, axis, keepdims=False):\n",
    "    return tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(t), axis=axis, keepdims=keepdims), keras.backend.epsilon()))\n",
    "\n",
    "def l1_norm(t, axis, keepdims=False):\n",
    "    return tf.reduce_sum(tf.abs(t), axis=axis, keepdims=keepdims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281eb8b",
   "metadata": {},
   "source": [
    "### Locality and context layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccca81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAggregationLayer(keras.layers.Layer):\n",
    "    def __init__(self, context_fan_in, type_position_dim, instance_position_dim, num_samples, initial_sharpness, train_context, dtype=None):\n",
    "        super(ContextAggregationLayer, self).__init__(dtype=dtype)\n",
    "\n",
    "        self.context_fan_in = context_fan_in\n",
    "        self.type_position_dim = type_position_dim\n",
    "        self.instance_position_dim = instance_position_dim\n",
    "        self.position_dim = type_position_dim + instance_position_dim\n",
    "        self.num_samples = num_samples\n",
    "        self.initial_sharpness = initial_sharpness\n",
    "        self.train_context = train_context\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.downstream_dim = input_shape[-1]\n",
    "        self.downstream_num_instances = input_shape[-2]\n",
    "        self.downstream_num_types = input_shape[-3]\n",
    "        self.downstream_num = self.downstream_num_instances * self.downstream_num_types\n",
    "\n",
    "        # Encodes the positions of downstream column types\n",
    "        # Positions are within a unit ball.\n",
    "        self.position_constraint = keras.constraints.MaxNorm(max_value=1, axis=1)\n",
    "        self.downstream_type_positions = self.add_weight(\n",
    "            shape=(self.downstream_num_types, self.type_position_dim),\n",
    "            initializer=self.initialize_positions,\n",
    "            name='type_positions',\n",
    "            trainable=True,\n",
    "            #use_resource=True,\n",
    "            constraint=self.position_constraint,\n",
    "        )\n",
    "        self.downstream_instance_positions = self.add_weight(\n",
    "            shape=(self.downstream_num_instances, self.instance_position_dim),\n",
    "            initializer=self.initialize_positions,\n",
    "            name='instance_positions',\n",
    "            trainable=True,\n",
    "            #use_resource=True,\n",
    "            constraint=self.position_constraint,\n",
    "        )\n",
    "        \n",
    "        # These rotations form the \"neighborhood\" of a given position.    \n",
    "        # Shape: position_dim -> position_dim * context_fan_in\n",
    "        self.context_queries = Dense(\n",
    "            self.position_dim * self.context_fan_in,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=self.initialize_rotations,\n",
    "            name='context_queries',\n",
    "            trainable=self.train_context,\n",
    "            kernel_constraint=self.constrain_to_rotations,\n",
    "        )\n",
    "        \n",
    "        self.context_sharpness = self.add_weight(\n",
    "            shape=(self.context_fan_in,),\n",
    "            initializer=keras.initializers.Constant(value=self.initial_sharpness),\n",
    "            name='context_sharpness',\n",
    "            trainable=True,\n",
    "            #use_resource=True,\n",
    "        )\n",
    "\n",
    "        super(ContextAggregationLayer, self).build(input_shape)\n",
    "    \n",
    "    def initialize_positions(self, shape, dtype):\n",
    "        rn = keras.initializers.TruncatedNormal()(shape, dtype=dtype)\n",
    "        # Initializing to smaller values initially helps positions converge faster.\n",
    "        return 0.1 * keras.constraints.UnitNorm(axis=1)(rn)\n",
    "    \n",
    "    def constrain_to_rotations(self, a):\n",
    "        rs = tf.split(a, self.context_fan_in, axis=1)\n",
    "        rs = tf.stack(rs)\n",
    "        s, u, v = tf.linalg.svd(rs)\n",
    "        vh = tf.linalg.adjoint(v)\n",
    "        constrained_rs = tf.matmul(u, vh)\n",
    "        \n",
    "        return tf.concat(tf.unstack(constrained_rs), axis=1)\n",
    "    \n",
    "    def initialize_rotations(self, shape, dtype):\n",
    "        identity = keras.initializers.Identity()((self.position_dim, self.position_dim), dtype=dtype)\n",
    "        rotations = []\n",
    "        # The first rotation is always the identity (i.e. the seed value itself)\n",
    "        rotations.append(identity)\n",
    "        for i in range(1, self.context_fan_in):\n",
    "            rotation = keras.initializers.Orthogonal(gain=0.1)((self.position_dim, self.position_dim), dtype=dtype)\n",
    "            rotations.append(identity + rotation)\n",
    "        rotations = tf.concat(rotations, axis=1)\n",
    "        return self.constrain_to_rotations(rotations)\n",
    "    \n",
    "    def sample_seed_indexes(self):\n",
    "        # This tensorflow sampling method is inspired by https://stackoverflow.com/a/54755281\n",
    "        # It approximates Numpy's np.random.choice with replace=False\n",
    "        # Sampling different seed indexes for each batch helps the positions and contexts\n",
    "        # to converge better to true neighborhoods. I was previously using a fixed sampling\n",
    "        # throughout the whole training, and it led to the seed positions to get stuck\n",
    "        # in specific values that didn't resemble their neighbors.\n",
    "\n",
    "        uniform_distribution = tf.random.uniform([self.downstream_num_types * self.downstream_num_instances], minval=0.0, maxval=1.0)\n",
    "        _, top_indexes = tf.nn.top_k(uniform_distribution, self.num_samples)\n",
    "\n",
    "        return top_indexes\n",
    "    \n",
    "    def combine_positions(self):\n",
    "        # Broadcast type positions over instances\n",
    "        # Shape: (num_types * num_instances, type_position_dim)\n",
    "        type_positions = tf.repeat(self.downstream_type_positions, self.downstream_num_instances, axis=0)\n",
    "        \n",
    "        # Shape: (num_types * num_instances, instance_position_dim)\n",
    "        instance_positions = tf.repeat(self.downstream_instance_positions, self.downstream_num_types, axis=0)\n",
    "        \n",
    "        # Shape: (..., num_types * num_instances, position_dim)\n",
    "        downstream_positions = tf.concat([type_positions, instance_positions], axis=-1)\n",
    "        downstream_positions = self.position_constraint(downstream_positions)\n",
    "        downstream_positions = tf.cast(downstream_positions, self.compute_dtype)\n",
    "        \n",
    "        return downstream_positions\n",
    "        \n",
    "    @tf.function(jit_compile=True)\n",
    "    def reconstruct(self, full_context):\n",
    "        downstream_positions = self.combine_positions()\n",
    "        \n",
    "        # Position shape: (..., num_columns, position_dim)\n",
    "        # Value shape: (..., num_columns, context_fan_in * downstream_dim)\n",
    "        position, values = tf.split(\n",
    "            full_context,\n",
    "            [\n",
    "                self.position_dim,\n",
    "                self.context_fan_in * self.downstream_dim,\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        \n",
    "        # Restore scale\n",
    "        values *= np.sqrt(self.context_fan_in)\n",
    "\n",
    "        # Shape: (context_fan_in, ..., num_columns, downstream_dim)\n",
    "        values = tf.stack(tf.split(values, self.context_fan_in, axis=-1), axis=0)\n",
    "\n",
    "        # Shape: (..., num_columns, context_fan_in * position_dim)\n",
    "        queries = self.context_queries(position)\n",
    "        \n",
    "        # Shape: (context_fan_in, ..., num_columns, position_dim)\n",
    "        queries = tf.stack(tf.split(queries, self.context_fan_in, axis=-1), axis=0)\n",
    "        \n",
    "        # Shape: (context_fan_in)\n",
    "        exp_sharpness = tf.exp(self.context_sharpness)\n",
    "        \n",
    "        # Needed so we can call reconstruct directly when in mixed precision\n",
    "        cast_downstream_positions = tf.cast(downstream_positions, self.compute_dtype)\n",
    "        exp_sharpness = tf.cast(exp_sharpness, self.compute_dtype)\n",
    "        queries = tf.cast(queries, self.compute_dtype)\n",
    "        values = tf.cast(values, self.compute_dtype)\n",
    "        \n",
    "        # Reconstruct by summing over the reconstruction heads\n",
    "        \n",
    "        # Shape: (downstream_num, position_dim), (context_fan_in, ..., num_columns, position_dim) -> (context_fan_in, ..., num_columns, downstream_num)\n",
    "        attention_similarities = tf.einsum('ND,H...SD->H...SN', cast_downstream_positions, queries)\n",
    "        # Scale attention similarities to exactly `sharpness` to maintain scale equivariance.\n",
    "        exp_sharpness = tf.reshape(exp_sharpness, [tf.shape(exp_sharpness)[0]] + [1] * (len(attention_similarities.shape) - 1))\n",
    "        attention_similarities = exp_sharpness * tf.math.l2_normalize(attention_similarities, axis=-1, epsilon=tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Shape: (context_fan_in, ..., num_columns, downstream_num) -> (context_fan_in, ..., num_columns, downstream_num)\n",
    "        attention_scores = tf.nn.softmax(attention_similarities, axis=-1)\n",
    "        \n",
    "        # Shape: (context_fan_in, ..., num_columns, downstream_dim), (context_fan_in, ..., num_columns, downstream_num) -> (context_fan_in, ..., downstream_num, downstream_dim)\n",
    "        head_outputs = tf.einsum('...CD,...CN->...ND', values, attention_scores)\n",
    "\n",
    "        flattened_reconstruction = tf.reduce_sum(tf.cast(head_outputs, full_context.dtype), axis=0)\n",
    "        \n",
    "        # Split flattened reconstruction back out into types and instances\n",
    "        reconstruction_shape = tf.concat([tf.shape(full_context)[0:-2], [self.downstream_num_types, self.downstream_num_instances, self.downstream_dim]], axis=0)\n",
    "        reconstruction = tf.reshape(flattened_reconstruction, reconstruction_shape)\n",
    "\n",
    "        return reconstruction\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def call(self, downstream_input, training=None):\n",
    "        # Shape of downstream_input: (..., num_types, num_instances, downstream_dim)\n",
    "        \n",
    "        downstream_positions = self.combine_positions()\n",
    "        \n",
    "        # Shape: (..., num_samples, position_dim)\n",
    "        seed_positions = tf.gather(downstream_positions, self.sample_seed_indexes(), axis=-2)\n",
    "\n",
    "        # Flatten all downstream inputs across types * instances.\n",
    "        # Shape: (..., downstream_num, position_dim)\n",
    "        flattened_downstream_input = tf.reshape(downstream_input, tf.concat([tf.shape(downstream_input)[:-3], [self.downstream_num, self.downstream_dim]], axis=0))\n",
    "        \n",
    "        # Shape: (..., num_samples, context_fan_in * position_dim)\n",
    "        queries = self.context_queries(seed_positions)\n",
    "        \n",
    "        # Shape: (..., context_fan_in, num_samples, position_dim)\n",
    "        queries = tf.stack(tf.split(queries, self.context_fan_in, axis=-1), axis=0)\n",
    "        \n",
    "        # Shape: (context_fan_in)\n",
    "        exp_sharpness = tf.exp(self.context_sharpness)\n",
    "        \n",
    "        # Cast for mixed precision\n",
    "        queries = tf.cast(queries, self.compute_dtype)\n",
    "        exp_sharpness = tf.cast(exp_sharpness, self.compute_dtype)\n",
    "\n",
    "        # Shape: (downstream_num, position_dim), (num_heads, num_samples, position_dim) -> (num_heads, num_samples, downstream_num)\n",
    "        attention_similarities = tf.einsum('ND,HSD->HSN', downstream_positions, queries)\n",
    "        # Scale attention similarities to exactly `sharpness` to maintain scale equivariance\n",
    "        exp_sharpness = tf.reshape(exp_sharpness, [tf.shape(exp_sharpness)[0]] + [1] * (len(attention_similarities.shape) - 1))\n",
    "        attention_similarities = exp_sharpness * tf.math.l2_normalize(attention_similarities, axis=-1, epsilon=keras.backend.epsilon())\n",
    "\n",
    "        # Shape: (num_heads. num_samples, downstream_num) -> (num_heads, num_samples, downstream_num)\n",
    "        attention_scores = tf.nn.softmax(attention_similarities, axis=-1)\n",
    "        \n",
    "        # Process each head individually to reduce the amount of GPU memory needed.\n",
    "        # Intentionally doing this in a Python loop instead of TensorFlow while_loop, because\n",
    "        # it appears that the use of TensorArray for storing results adds some significant overhead.\n",
    "        # This makes the processing graph a bit bigger, but is not a huge deal as long as the context_fan_in\n",
    "        # is of a reasonable size. This also allows us to apply jit_compile to the entire call() function.\n",
    "        head_values = []\n",
    "        for head_idx in range(0, self.context_fan_in):\n",
    "            # Shape: (..., downstream_num, downstream_dim), (num_samples, downstream_num) -> (..., num_samples, downstream_dim)\n",
    "            head_output = tf.matmul(attention_scores[head_idx], flattened_downstream_input)\n",
    "            # Restore scale\n",
    "            head_output /= tf.cast(np.sqrt(self.context_fan_in), self.compute_dtype)\n",
    "            head_values.append(head_output)\n",
    "        \n",
    "        # Shape: (..., num_samples, position_dim + context_fan_in * downstream_dim)\n",
    "        expanded_seed_positions = tf.broadcast_to(\n",
    "            seed_positions,\n",
    "            tf.concat([tf.shape(head_values[0])[0:-1], [self.position_dim]], axis=0),\n",
    "        )\n",
    "        full_context = tf.concat([expanded_seed_positions] + head_values, axis=-1)\n",
    "        \n",
    "        return full_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d2ad2",
   "metadata": {},
   "source": [
    "### Coding / decoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a9a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_full_reconstruction_error(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Like cosine similarity, but weighted by the norm of the true vectors\n",
    "    similarity = tf.reduce_sum(y_true * tf.math.l2_normalize(y_pred, axis=-1, epsilon=keras.backend.epsilon()), axis=-1)\n",
    "    weights = stable_norm(y_true, axis=-1)\n",
    "\n",
    "    # Quadratic formulation. The idea behind this is to put more emphasis on reconstructing vectors *approximately*\n",
    "    # correctly, rather than exactly correctly.\n",
    "    return 4 * tf.reduce_mean(tf.square(weights - similarity))\n",
    "    # Linear formulation\n",
    "    #return tf.reduce_mean(weights - similarity)\n",
    "\n",
    "def compute_partial_reconstruction_error(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Compare the L1 norm size change when adding y_pred vectors to y_true vectors\n",
    "    norm_y_true = y_true / tf.maximum(l1_norm(y_true, axis=-1, keepdims=True), keras.backend.epsilon())\n",
    "    norm_y_pred = y_pred / tf.maximum(l1_norm(y_pred, axis=-1, keepdims=True), keras.backend.epsilon())\n",
    "    weights = stable_norm(y_true, axis=-1)\n",
    "    partial_loss = weights * (2.0 - l1_norm(norm_y_true + norm_y_pred, axis=-1))\n",
    "    \n",
    "    return tf.reduce_mean(partial_loss)\n",
    "\n",
    "def compute_scale_error(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.stop_gradient(y_true), tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Alternative formulation: Scale in the \"correct direction\" (norm of differences)\n",
    "    #diffs = stable_norm(y_true - y_pred, axis=-1)\n",
    "    \n",
    "    # Traditional formulation (scale only):\n",
    "    # Penalize scale differences between y_true and y_pred\n",
    "    diffs = tf.abs(stable_norm(y_true, -1) - stable_norm(y_pred, -1))\n",
    "\n",
    "    return tf.reduce_mean(diffs)\n",
    "\n",
    "def mean_l1_activity(t):\n",
    "    t = tf.cast(t, tf.float32)\n",
    "    # Sum over a) the components of each instance, b) the instances. Then average.\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(tf.abs(t), axis=-1), axis=-1))\n",
    "\n",
    "def mean_l2_activity(t):\n",
    "    t = tf.cast(t, tf.float32)\n",
    "    # Sum over a) the components of each instance, b) the instances. Then average.\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(tf.square(t), axis=-1), axis=-1))\n",
    "\n",
    "class PermanenceSeparatorLayer(keras.layers.Layer):\n",
    "    def __init__(self, upstream_dim, position_dim, noise_rate, dtype=None):\n",
    "        super(PermanenceSeparatorLayer, self).__init__(dtype=dtype)\n",
    "        \n",
    "        self.upstream_dim = upstream_dim\n",
    "        self.position_dim = position_dim\n",
    "        self.noise_rate = noise_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.internal_dim = input_shape[-1]\n",
    "\n",
    "        separation_initializer_stddev = np.sqrt(2 / self.internal_dim)\n",
    "        reconstruction_initializer_stddev = 0.1 * np.sqrt(2 / self.upstream_dim)\n",
    "\n",
    "        self.separation_layers = [\n",
    "            Dense(\n",
    "                np.ceil(2 * self.internal_dim / 8.0) * 8,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=keras.initializers.TruncatedNormal(stddev=separation_initializer_stddev),\n",
    "                name='separation_layer_1',\n",
    "            ),\n",
    "            LeakyReLU(alpha=0.1),\n",
    "            Dense(\n",
    "                np.ceil(2 * self.internal_dim / 8.0) * 8,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=keras.initializers.TruncatedNormal(stddev=separation_initializer_stddev),\n",
    "                name='separation_layer_2',\n",
    "            ),\n",
    "            LeakyReLU(alpha=0.1),\n",
    "            Dense(\n",
    "                self.upstream_dim,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=keras.initializers.TruncatedNormal(stddev=separation_initializer_stddev),\n",
    "                name='separation_layer_3',\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "        self.reconstruction_layers = [\n",
    "            Dense(\n",
    "                np.ceil(2 * self.internal_dim / 8.0) * 8,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=keras.initializers.TruncatedNormal(stddev=reconstruction_initializer_stddev),\n",
    "                name='reconstruction_layer_1',\n",
    "            ),\n",
    "            LeakyReLU(alpha=0.1),\n",
    "            Dense(\n",
    "                np.ceil(2 * self.internal_dim / 8.0) * 8,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=keras.initializers.TruncatedNormal(stddev=reconstruction_initializer_stddev),\n",
    "                name='reconstruction_layer_2',\n",
    "            ),\n",
    "            LeakyReLU(alpha=0.1),\n",
    "            Dense(\n",
    "                self.internal_dim,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=keras.initializers.TruncatedNormal(stddev=reconstruction_initializer_stddev),\n",
    "                name='reconstruction_layer_3',\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        super(PermanenceSeparatorLayer, self).build(input_shape)\n",
    "     \n",
    "    @tf.function(jit_compile=True)\n",
    "    def reconstruct(self, separated_input, training=False):\n",
    "        # Inject noise to increase robustness.\n",
    "        if self.noise_rate and training:\n",
    "            noise = tf.random.normal(\n",
    "                shape=tf.shape(separated_input),\n",
    "                mean=0.0,\n",
    "                stddev=self.noise_rate/np.sqrt(self.upstream_dim),\n",
    "                dtype=separated_input.dtype,\n",
    "            )\n",
    "            separated_input += tf.stop_gradient(noise)\n",
    "        \n",
    "        enriched_input = separated_input\n",
    "        for l in self.reconstruction_layers:\n",
    "            enriched_input = l(enriched_input)\n",
    "        \n",
    "        return enriched_input\n",
    "\n",
    "    def call(self, enriched_input, training=None):        \n",
    "        separated_input = enriched_input\n",
    "        for l in self.separation_layers:\n",
    "            separated_input = l(separated_input)\n",
    "\n",
    "        return separated_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291cfbe",
   "metadata": {},
   "source": [
    "### Single column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3583dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelectorLayer(keras.layers.Layer):\n",
    "    def __init__(self, num_column_instances, selection_initial_sharpness, dtype=None):\n",
    "        super(ColumnSelectorLayer, self).__init__(dtype=dtype)\n",
    "    \n",
    "        self.num_column_instances = num_column_instances\n",
    "        self.selection_initial_sharpness = selection_initial_sharpness\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.upstream_dim = input_shape[-1]\n",
    "\n",
    "        self.selection_sharpness = self.add_weight(\n",
    "            shape=(1,),\n",
    "            initializer=keras.initializers.Constant(self.selection_initial_sharpness),\n",
    "            name='selection_sharpness',\n",
    "            #use_resource=True,\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        self.suppression_sharpness = self.add_weight(\n",
    "            shape=(1,),\n",
    "            # Initialize to a bit less than selection sharpness.\n",
    "            initializer=keras.initializers.Constant(self.selection_initial_sharpness - 1.0),\n",
    "            name='suppression_sharpness',\n",
    "            #use_resource=True,\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        # How much of the previous instance activation we add to the queries at the next\n",
    "        # time step. This is to facilitate \"lock on\" behavior to improve permanence.\n",
    "        self.stickiness = self.add_weight(\n",
    "            shape=(1,),\n",
    "            initializer=keras.initializers.Constant(0.1),\n",
    "            constraint=keras.constraints.NonNeg(),\n",
    "            name='stickiness',\n",
    "            #use_resource=True,\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        super(ColumnSelectorLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, upstream_candidates, training=None):\n",
    "        # We propagate selected inputs through time, to enable \"locking on\" to a particular input.\n",
    "        num_time_steps = tf.shape(upstream_candidates)[1]\n",
    "        selected_inputs_shape_at_t = [tf.shape(upstream_candidates)[0], self.num_column_instances, self.upstream_dim]\n",
    "        \n",
    "        previous_selected_inputs = tf.zeros(selected_inputs_shape_at_t, dtype=upstream_candidates.dtype)\n",
    "        selected_inputs_buffer = tf.TensorArray(dtype=upstream_candidates.dtype, size=num_time_steps, clear_after_read=True)\n",
    "        for t_idx in tf.range(0, num_time_steps):\n",
    "            upstream_candidates_at_t = upstream_candidates[:, t_idx, :, :]\n",
    "            \n",
    "            # We want to select num_column_instances samples that cover as much of the\n",
    "            # sample's values as possible, weighted by their magnitudes (i.e. samples with larger magnitude\n",
    "            # are more important to cover than those with smaller magnitude).\n",
    "            #\n",
    "            # We do this by using an iterative approach that resembles local suppression, similar to\n",
    "            # k-means clustering but differentiable.\n",
    "            #\n",
    "            # As an additional consideration, we also utilize selections from t-1 to implement a form of\n",
    "            # tracking. Once a certain sample has been selected at time t for a column instance, we try to\n",
    "            # \"lock on\" to that sample over subsequent time steps.\n",
    "\n",
    "            # Keep track of the remaining magnitudes of the candidates.\n",
    "            candidate_magnitudes = stable_norm(upstream_candidates_at_t, axis=-1)\n",
    "            remaining_scales = tf.ones_like(candidate_magnitudes)\n",
    "            instance_values_at_t_buffer = tf.TensorArray(dtype=upstream_candidates.dtype, size=self.num_column_instances, clear_after_read=True)\n",
    "            for inst_idx in tf.range(0, self.num_column_instances):\n",
    "                # Track the previous selection\n",
    "                tracking_selection_weights = tf.einsum(\n",
    "                    '...SD,...D->...S',\n",
    "                    upstream_candidates_at_t,\n",
    "                    previous_selected_inputs[:, inst_idx],\n",
    "                )\n",
    "                \n",
    "                # Combine it with a highest-magnitude selection mechanism and discount any instances that have\n",
    "                # already been covered by previous selections.\n",
    "                raw_selection_weights = remaining_scales * (candidate_magnitudes + self.stickiness * tracking_selection_weights)\n",
    "\n",
    "                # Compute the value for this instance\n",
    "                selection_weights = tf.nn.softmax(tf.math.exp(self.selection_sharpness) * raw_selection_weights, axis=-1)\n",
    "                instance_value_at_t = tf.einsum(\n",
    "                    '...S,...SD->...D',\n",
    "                    selection_weights,\n",
    "                    tf.expand_dims(remaining_scales, axis=-1) * upstream_candidates_at_t,\n",
    "                )\n",
    "\n",
    "                # Determine reduced scales for next instances,\n",
    "                # subtracting any weights that have been \"covered\" by this instance.\n",
    "                # Calculate coverage by rescaling the raw_selection_weights such that the highest weight becomes 1.\n",
    "                coverage = tf.nn.softmax(tf.math.exp(self.suppression_sharpness) * raw_selection_weights, axis=-1)\n",
    "                coverage = coverage / tf.maximum(tf.reduce_max(coverage, axis=-1, keepdims=True), keras.backend.epsilon())\n",
    "                remaining_scales = (1.0 - coverage) * remaining_scales\n",
    "                \n",
    "                instance_values_at_t_buffer = instance_values_at_t_buffer.write(inst_idx, instance_value_at_t)\n",
    "\n",
    "            instance_values_at_t = instance_values_at_t_buffer.stack()\n",
    "            instance_values_at_t = tf.transpose(instance_values_at_t, perm=[1, 0, 2])\n",
    "\n",
    "            previous_selected_inputs = instance_values_at_t\n",
    "            selected_inputs_buffer = selected_inputs_buffer.write(t_idx, instance_values_at_t)\n",
    "\n",
    "        # Concatenate the results for each time index back together into a single tensor\n",
    "        # Shape: (num_time_steps, ..., num_column_instances, upstream_dim)\n",
    "        selected_inputs = selected_inputs_buffer.stack()\n",
    "        # Shape: (..., num_time_steps, num_column_instances, upstream_dim)\n",
    "        selected_inputs = tf.transpose(selected_inputs, perm=[1, 0, 2, 3])\n",
    "\n",
    "        return selected_inputs\n",
    "        \n",
    "class ColumnLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        upstream_dim,\n",
    "        num_column_instances,\n",
    "        type_position_dim,\n",
    "        instance_position_dim,\n",
    "        context_fan_in,\n",
    "        context_num_samples,\n",
    "        context_initial_sharpness,\n",
    "        train_context,\n",
    "        noise_rate,\n",
    "        activity_reg,\n",
    "        selection_initial_sharpness,\n",
    "    ):\n",
    "        super(ColumnLayer, self).__init__()\n",
    "\n",
    "        self.upstream_dim = upstream_dim\n",
    "        self.num_column_instances = num_column_instances\n",
    "        self.type_position_dim = type_position_dim\n",
    "        self.instance_position_dim = instance_position_dim\n",
    "        self.context_fan_in = context_fan_in\n",
    "        self.context_num_samples = context_num_samples\n",
    "        self.context_initial_sharpness = context_initial_sharpness\n",
    "        self.train_context = train_context\n",
    "        self.noise_rate = noise_rate\n",
    "        self.activity_reg = activity_reg\n",
    "        self.selection_initial_sharpness = selection_initial_sharpness\n",
    "    \n",
    "    def build(self, input_shape):        \n",
    "        self.context_aggregator = ContextAggregationLayer(\n",
    "            context_fan_in=self.context_fan_in,\n",
    "            type_position_dim=self.type_position_dim,\n",
    "            instance_position_dim=self.instance_position_dim,\n",
    "            num_samples=self.context_num_samples,\n",
    "            initial_sharpness=self.context_initial_sharpness,\n",
    "            train_context=self.train_context,\n",
    "            # Note: mixed_float16 is unstable and leads to nans, unless loss scaling is used. However,\n",
    "            #   convergence does not happen with loss scaling for some reason.\n",
    "            #   bfloat16 works better.\n",
    "            dtype='mixed_bfloat16',\n",
    "        )\n",
    "        \n",
    "        self.permanence_separator = PermanenceSeparatorLayer(\n",
    "            upstream_dim=self.upstream_dim,\n",
    "            position_dim=self.type_position_dim + self.instance_position_dim,\n",
    "            noise_rate=self.noise_rate,\n",
    "            #dtype='mixed_bfloat16',\n",
    "        )\n",
    "        \n",
    "        self.column_selector = ColumnSelectorLayer(\n",
    "            num_column_instances=self.num_column_instances,\n",
    "            selection_initial_sharpness=self.selection_initial_sharpness,\n",
    "            #dtype='mixed_bfloat16',\n",
    "        )\n",
    "        \n",
    "        super(ColumnLayer, self).build(input_shape)\n",
    "    \n",
    "    def reconstruct(self, upstream_state):\n",
    "        reconstruction = self.permanence_separator.reconstruct(upstream_state)\n",
    "        reconstruction = self.context_aggregator.reconstruct(reconstruction)\n",
    "        return reconstruction\n",
    "    \n",
    "    def call(self, downstream_input, training=None):\n",
    "        # Aggregate contexts\n",
    "        enriched_input = self.context_aggregator(downstream_input)\n",
    "        \n",
    "        # Compute upstream state candidates by converting inputs into their permanence-separated representations\n",
    "        upstream_candidates = self.permanence_separator(enriched_input)\n",
    "\n",
    "        # Select a finite set of them\n",
    "        upstream_state = self.column_selector(upstream_candidates)\n",
    "\n",
    "        # Compute a reconstruction of the downstream input\n",
    "        reconstruction = self.permanence_separator.reconstruct(upstream_state, training=training)\n",
    "        reconstruction = self.context_aggregator.reconstruct(reconstruction)\n",
    "        \n",
    "        # Try to keep the upstream state sparse if we can - this makes the work of upstream layers easier,\n",
    "        # and encourages the encoder to identify distinct recurring patterns in the inputs.\n",
    "        activity_reg = mean_l1_activity(upstream_state)\n",
    "        self.add_metric(activity_reg, 'mean_l1_activity')\n",
    "        self.add_loss(self.activity_reg * activity_reg)\n",
    "        \n",
    "        return [upstream_state, reconstruction]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c588f4e",
   "metadata": {},
   "source": [
    "### Multi-column overall layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3672a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_change_non_sparseness(cur_state, future_states, bias, diff_scale_factor):    \n",
    "    diff = future_states - cur_state\n",
    "    l1 = l1_norm(diff, axis=-1)\n",
    "    # Minimum is to avoid negative values due to the espilon in stable_norm\n",
    "    l2 = tf.minimum(l1, stable_norm(diff, axis=-1))\n",
    "\n",
    "    non_sparseness = (l1 + bias) / (l2 + bias) - 1.0\n",
    "    \n",
    "    # Also slightly penalize the absolute scale of the difference.\n",
    "    # If we don't have this regularizer, then networks can easily learn to bypass the permanence loss\n",
    "    # by creating a single component that responds to minor noise and changes with large magnitude, thereby\n",
    "    # making change always look sparse.\n",
    "    lin_scale = l1\n",
    "    sq_scale = tf.reduce_sum(tf.square(diff), axis=-1)\n",
    "    \n",
    "    return non_sparseness + diff_scale_factor * (lin_scale + sq_scale)\n",
    "\n",
    "def compute_permanence_loss(cur_upstream_state, future_upstream_states, cur_reconstruction, future_reconstructions):\n",
    "    cur_upstream_state = tf.cast(cur_upstream_state, tf.float32)\n",
    "    future_upstream_states = tf.cast(future_upstream_states, tf.float32)\n",
    "    # Reconstruction differences are used for normalization of the permanence loss. The bigger the\n",
    "    # reconstruction difference, the more weight we give the the permanence loss. This supports early\n",
    "    # convergence of the network, as it only starts applying permanence loss pressure as the network\n",
    "    # starts reconstructing differences between states.\n",
    "    cur_reconstruction = tf.cast(tf.stop_gradient(cur_reconstruction), tf.float32)\n",
    "    future_reconstructions = tf.cast(tf.stop_gradient(future_reconstructions), tf.float32)\n",
    "    \n",
    "    bias = 0.1\n",
    "    diff_scale_factor = 0.1\n",
    "    \n",
    "    upstream_non_sparseness = tf.reduce_mean(\n",
    "        compute_change_non_sparseness(\n",
    "            cur_upstream_state,\n",
    "            future_upstream_states,\n",
    "            bias=bias,\n",
    "            diff_scale_factor=diff_scale_factor,\n",
    "        ),\n",
    "        axis=-1,\n",
    "    )\n",
    "    \n",
    "    reconstruction_diff_scale = tf.reduce_mean(stable_norm(future_reconstructions - cur_reconstruction, axis=-1))\n",
    "    \n",
    "    scaled_non_sparseness = reconstruction_diff_scale * upstream_non_sparseness\n",
    "    \n",
    "    return tf.reduce_mean(scaled_non_sparseness)\n",
    "\n",
    "class PtolemyLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_types,\n",
    "        num_instances_per_type,\n",
    "        context_fan_in,\n",
    "        upstream_dim,\n",
    "        type_position_dim,\n",
    "        instance_position_dim,\n",
    "        stride=1,\n",
    "        permanence_loss_rho=0.2,\n",
    "        context_initial_sharpness=3.0,\n",
    "        noise_rate=0.2,\n",
    "        train_context=True,\n",
    "        add_default=False,\n",
    "        train_default=True,\n",
    "        activity_reg=0.001,\n",
    "        scale_error_coefficient=0.05,\n",
    "        selection_initial_sharpness=3.0,\n",
    "    ):\n",
    "        super(PtolemyLayer, self).__init__()\n",
    "        \n",
    "        self.num_types = num_types\n",
    "        self.num_instances_per_type = num_instances_per_type\n",
    "        self.context_fan_in = context_fan_in\n",
    "        self.upstream_dim = upstream_dim\n",
    "        self.type_position_dim = type_position_dim\n",
    "        self.instance_position_dim = instance_position_dim\n",
    "        self.stride = stride\n",
    "        self.permanence_loss_rho = permanence_loss_rho\n",
    "        self.context_initial_sharpness = context_initial_sharpness\n",
    "        self.noise_rate = noise_rate\n",
    "        self.train_context = train_context\n",
    "        self.add_default = add_default\n",
    "        self.train_default = train_default\n",
    "        self.activity_reg = activity_reg\n",
    "        self.scale_error_coefficient = scale_error_coefficient\n",
    "        self.selection_initial_sharpness = selection_initial_sharpness\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'num_types': self.num_types,\n",
    "            'num_instances_per_type': self.num_instances_per_type,\n",
    "            'context_fan_in': self.context_fan_in,\n",
    "            'upstream_dim': self.upstream_dim,\n",
    "            'type_position_dim': self.type_position_dim,\n",
    "            'instance_position_dim': self.instance_position_dim,\n",
    "            'stride': self.stride,\n",
    "            'permanence_loss_rho': self.permanence_loss_rho,\n",
    "            'context_initial_sharpness': self.context_initial_sharpness,\n",
    "            'noise_rate': self.noise_rate,\n",
    "            'train_context': self.train_context,\n",
    "            'add_default': self.add_default,\n",
    "            'train_default': self.train_default,\n",
    "            'activity_reg': self.activity_reg,\n",
    "            'scale_error_coefficient': self.scale_error_coefficient,\n",
    "            'selection_initial_sharpness': self.selection_initial_sharpness,\n",
    "        }\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.downstream_dim = input_shape[-1]\n",
    "        self.downstream_num_instances = input_shape[-2]\n",
    "        self.downstream_num_types = input_shape[-3]\n",
    "        \n",
    "        downstream_num = self.downstream_num_types * self.downstream_num_instances\n",
    "        \n",
    "        num_samples = int(downstream_num / self.stride)\n",
    "        \n",
    "        self.columns = []\n",
    "        for c in range(0, self.num_types):\n",
    "            self.columns.append(\n",
    "                ColumnLayer(\n",
    "                    upstream_dim=self.upstream_dim,\n",
    "                    num_column_instances=self.num_instances_per_type,\n",
    "                    type_position_dim=self.type_position_dim,\n",
    "                    instance_position_dim=self.instance_position_dim,\n",
    "                    context_fan_in=self.context_fan_in,\n",
    "                    context_num_samples=num_samples,\n",
    "                    context_initial_sharpness=self.context_initial_sharpness,\n",
    "                    train_context=self.train_context,\n",
    "                    noise_rate=self.noise_rate,\n",
    "                    activity_reg=self.activity_reg,\n",
    "                    selection_initial_sharpness=self.selection_initial_sharpness,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        if self.add_default:\n",
    "            self.default = self.add_weight(\n",
    "                shape=(self.downstream_num_types, self.downstream_num_instances, self.downstream_dim),\n",
    "                trainable=self.train_default,\n",
    "                initializer=keras.initializers.Zeros(),\n",
    "                #use_resource=True,\n",
    "                name='default',\n",
    "            )\n",
    "        else:\n",
    "            self.default = None\n",
    "        \n",
    "        super(PtolemyLayer, self).build(input_shape)\n",
    "    \n",
    "    @tf.function(jit_compile=True)\n",
    "    def reconstruct(self, upstream_state):\n",
    "        type_reconstructions = []\n",
    "        for c in range(0, self.num_types):\n",
    "            type_reconstruction = self.columns[c].reconstruct(upstream_state[:, :, c, :, :])\n",
    "            type_reconstructions.append(type_reconstruction)\n",
    "            \n",
    "        reconstruction = self.reconstruct_from_type_reconstructions(type_reconstructions)\n",
    "        \n",
    "        return reconstruction\n",
    "    \n",
    "    def reconstruct_from_type_reconstructions(self, type_reconstructions):       \n",
    "        reconstruction = tf.math.add_n(type_reconstructions)\n",
    "\n",
    "        if self.add_default:\n",
    "            reconstruction += tf.cast(self.default, self.compute_dtype)\n",
    "        \n",
    "        return reconstruction\n",
    "    \n",
    "    def shuffle_instances(self, downstream_input):\n",
    "        # Shuffle each entry in the batch separately, but maintain consistent instance\n",
    "        # ordering within one sequence so that permanence loss can be computed correctly.\n",
    "        # Implementation inspired by Michael Mezher https://stackoverflow.com/a/74870471\n",
    "        batch_size = tf.shape(downstream_input)[0]\n",
    "        num_instances = tf.shape(downstream_input)[3]\n",
    "        rnd = tf.argsort(tf.random.uniform([batch_size, num_instances]), axis=-1)\n",
    "        rnd = tf.expand_dims(rnd, axis=1)\n",
    "        rnd = tf.expand_dims(rnd, axis=1)\n",
    "        rnd = tf.broadcast_to(rnd, tf.shape(downstream_input)[:4])\n",
    "        shuffled_downstream_input = tf.gather(downstream_input, rnd, batch_dims=3)\n",
    "        return shuffled_downstream_input\n",
    "    \n",
    "    def call(self, downstream_input, training=None):\n",
    "        # During training, shuffle the downstream instances to encourage instance permutation invariance\n",
    "        if training:\n",
    "            downstream_input = self.shuffle_instances(downstream_input)\n",
    "        \n",
    "        if self.add_default and not self.train_default:\n",
    "            # Update default to be the mean of the inputs\n",
    "            batch_mean = tf.reduce_mean(tf.reduce_mean(tf.cast(downstream_input, tf.float32), axis=0), axis=0)\n",
    "            if training:\n",
    "                self.default.assign(0.9 * tf.cast(self.default, tf.float32) + 0.1 * batch_mean)\n",
    "    \n",
    "        upstream_states = []\n",
    "        type_reconstructions = []\n",
    "        for c in range(0, self.num_types):\n",
    "            upstream_state, type_reconstruction = self.columns[c](\n",
    "                downstream_input,\n",
    "            )\n",
    " \n",
    "            upstream_states.append(upstream_state)\n",
    "            type_reconstructions.append(type_reconstruction)\n",
    "\n",
    "        upstream_state = tf.stack(upstream_states, axis=-3)\n",
    "        reconstruction = self.reconstruct_from_type_reconstructions(type_reconstructions)\n",
    "\n",
    "        if self.permanence_loss_rho:\n",
    "            # Compute pairwise permanence losses of all time steps compared to the 0th step\n",
    "            permanence_loss = compute_permanence_loss(\n",
    "                upstream_state[:, :1],\n",
    "                upstream_state[:, 1:],\n",
    "                reconstruction[:, :1],\n",
    "                reconstruction[:, 1:],\n",
    "            )\n",
    "            self.add_metric(permanence_loss, 'permanence_loss')\n",
    "            self.add_loss(self.permanence_loss_rho * permanence_loss)\n",
    "        \n",
    "        reconstruction_error = compute_full_reconstruction_error(tf.stop_gradient(downstream_input), reconstruction)\n",
    "        self.add_metric(reconstruction_error, 'reconstruction_error')\n",
    "        self.add_loss(reconstruction_error)\n",
    "        \n",
    "        scale_error = compute_scale_error(downstream_input, reconstruction)\n",
    "        self.add_metric(scale_error, 'scale_error')\n",
    "        self.add_loss(self.scale_error_coefficient * scale_error)\n",
    "\n",
    "        return [upstream_state, reconstruction]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26527a4",
   "metadata": {},
   "source": [
    "# Animation callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edd32c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimateReconstruction(keras.callbacks.Callback):\n",
    "    def __init__(self, layers, sequence, output_filename, frames_per_epoch=5):\n",
    "        self._layers = layers\n",
    "        self._sequence = sequence\n",
    "        self._num_frames = tf.shape(sequence)[0]\n",
    "        self._output_filename = output_filename\n",
    "        self._frames_per_epoch = frames_per_epoch\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self._input_frames = []\n",
    "        self._reconstruction_frames = []\n",
    "        self._upstream_frames = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self._animate(epoch)\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self._save_animation(self._reconstruction_frames, self._output_filename, is_img=True)\n",
    "        filename, extension = os.path.splitext(self._output_filename)\n",
    "        upstream_state_filename = filename + \"_states\" + extension\n",
    "        self._save_animation(self._upstream_frames, upstream_state_filename, is_img=False)\n",
    "        filename, extension = os.path.splitext(self._output_filename)\n",
    "        input_filename = filename + \"_input\" + extension\n",
    "        self._save_animation(self._input_frames, input_filename, is_img=True)\n",
    "    \n",
    "    def _animate(self, step):\n",
    "        sub_sequence = []\n",
    "        for frame_idx in range(step * self._frames_per_epoch, (step + 1) * self._frames_per_epoch):\n",
    "            sub_sequence.append(self._sequence[frame_idx % self._num_frames])\n",
    "        sub_sequence = tf.stack(sub_sequence, axis=0)\n",
    "        # Add a batch dimension\n",
    "        batch = tf.expand_dims(sub_sequence, 0)\n",
    "        upstream_state = batch\n",
    "        for l in self._layers:\n",
    "            upstream_state, _ = l(upstream_state)\n",
    "            \n",
    "        reconstruction = upstream_state\n",
    "        for l in reversed(self._layers):\n",
    "            reconstruction = l.reconstruct(reconstruction)\n",
    "        \n",
    "        for frame_idx in range(self._frames_per_epoch):\n",
    "            # Store inputs\n",
    "            frame_im_array = vecs_to_img(tf.squeeze(batch[0][frame_idx]).numpy())\n",
    "            frame_im = keras.utils.array_to_img(frame_im_array, scale=False)\n",
    "            self._input_frames.append((step, frame_im))\n",
    "            \n",
    "            # Plot upstream state\n",
    "            # Flatten types and type instances into a single dimension\n",
    "            shape = upstream_state.shape\n",
    "            flattened_upstream_state = tf.reshape(upstream_state[0][frame_idx], (-1, shape[-1])).numpy()\n",
    "            self._upstream_frames.append((step, flattened_upstream_state))\n",
    "            \n",
    "            # Plot reconstruction\n",
    "            frame_im_array = vecs_to_img(tf.squeeze(reconstruction[0][frame_idx]).numpy())\n",
    "            frame_im = keras.utils.array_to_img(frame_im_array, scale=False)\n",
    "            self._reconstruction_frames.append((step, frame_im))\n",
    "            \n",
    "    def _save_animation(self, frames, output_filename, is_img):\n",
    "        fig = plt.figure()\n",
    "        if is_img:\n",
    "            plot_fun = lambda f: (\n",
    "                plt.clf(),\n",
    "                plt.imshow(f[1], cmap='gray', vmin=0, vmax=255),\n",
    "                plt.figtext(0.1, 0.0, f\"epoch={f[0]}\", size=12.0),\n",
    "            )\n",
    "        else:\n",
    "            vmin = None\n",
    "            vmax = None\n",
    "            for _, f in frames:\n",
    "                vmin_f = np.min(f)\n",
    "                vmax_f = np.max(f)\n",
    "                if vmin is None or vmin_f < vmin:\n",
    "                    vmin = vmin_f\n",
    "                if vmax is None or vmax_f > vmax:\n",
    "                    vmax = vmax_f\n",
    "            plot_fun = lambda f: (\n",
    "                plt.clf(),\n",
    "                plt.imshow(f[1], vmin=vmin, vmax=vmax),\n",
    "                plt.figtext(0.1, 0.0, f\"epoch={f[0]}\", size=12.0),\n",
    "            )\n",
    "        anim = animation.FuncAnimation(\n",
    "            fig,\n",
    "            plot_fun,\n",
    "            frames=frames,\n",
    "            interval=1,\n",
    "            blit=False,\n",
    "        )\n",
    "        anim.save(output_filename, fps=15, writer='imagemagick')\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a24baf",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f8082b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(3, x_size * y_size, 1, 2))\n",
    "l1 = PtolemyLayer(\n",
    "   num_types=1,\n",
    "   num_instances_per_type=8,\n",
    "   context_fan_in=32,\n",
    "   upstream_dim=40,\n",
    "   type_position_dim=32,\n",
    "   instance_position_dim=0,\n",
    "   stride=5,\n",
    "   permanence_loss_rho=0.2,\n",
    "   context_initial_sharpness=3.0,\n",
    "   add_default=True,\n",
    "   noise_rate=0.2,\n",
    ")\n",
    "\n",
    "upstream_state_1, downstream_reconstruction_1 = l1(inp)\n",
    "\n",
    "upstream_state = upstream_state_1\n",
    "downstream_reconstruction = downstream_reconstruction_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2549902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 3, 2500, 1, 2)]   0         \n",
      "                                                                 \n",
      " ptolemy_layer_2 (PtolemyLa  [(None, None, 1, 8, 40)   243755    \n",
      " yer)                        , (None, None, 2500, 1,             \n",
      "                              2)]                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 243755 (952.17 KB)\n",
      "Trainable params: 243755 (952.17 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reconstruction_model = keras.models.Model(inputs=[inp], outputs=downstream_reconstruction)\n",
    "upstream_state_model = keras.models.Model(inputs=[inp], outputs=upstream_state)\n",
    "\n",
    "@tf.function\n",
    "def zero_loss(t, p):\n",
    "    return 0.0\n",
    "\n",
    "optimizer = keras.optimizers.Adam(\n",
    "    learning_rate = 0.001,\n",
    "    #clipnorm=0.1,\n",
    ")\n",
    "#optimizer = keras.mixed_precision.LossScaleOptimizer(optimizer)  # Not needed for mixed_bfloat16\n",
    "reconstruction_model.compile(\n",
    "    loss = zero_loss,\n",
    "    optimizer = optimizer,\n",
    "    metrics = ['cosine_similarity'],\n",
    ")\n",
    "reconstruction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b3b2cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 12:23:06.259860: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:103] Profiler session initializing.\n",
      "2025-04-04 12:23:06.259875: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:118] Profiler session started.\n",
      "2025-04-04 12:23:06.269915: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:130] Profiler session tear down.\n",
      "I0000 00:00:1743794586.295083   53189 cupti_tracer.cc:1249] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 12:23:18.068970: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_3', 868 bytes spill stores, 868 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_2', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 47s 288ms/step - loss: 0.5764 - cosine_similarity: 0.7922 - permanence_loss: 9.3602e-05 - reconstruction_error: 0.5516 - scale_error: 0.9915 - mean_l1_activity: 4.8592\n",
      "Epoch 2/128\n",
      "125/125 [==============================] - 39s 314ms/step - loss: 0.5341 - cosine_similarity: 0.8095 - permanence_loss: 7.7700e-05 - reconstruction_error: 0.5122 - scale_error: 0.9909 - mean_l1_activity: 2.0590\n",
      "Epoch 3/128\n",
      "125/125 [==============================] - 46s 370ms/step - loss: 0.5228 - cosine_similarity: 0.8175 - permanence_loss: 1.7642e-04 - reconstruction_error: 0.4999 - scale_error: 0.9900 - mean_l1_activity: 3.1103\n",
      "Epoch 4/128\n",
      "125/125 [==============================] - 41s 326ms/step - loss: 0.5062 - cosine_similarity: 0.8289 - permanence_loss: 2.0878e-04 - reconstruction_error: 0.4828 - scale_error: 0.9903 - mean_l1_activity: 3.5844\n",
      "Epoch 5/128\n",
      "125/125 [==============================] - 48s 382ms/step - loss: 0.4913 - cosine_similarity: 0.8380 - permanence_loss: 3.6563e-04 - reconstruction_error: 0.4673 - scale_error: 0.9899 - mean_l1_activity: 4.0892\n",
      "Epoch 6/128\n",
      "125/125 [==============================] - 45s 357ms/step - loss: 0.4721 - cosine_similarity: 0.8500 - permanence_loss: 6.5546e-04 - reconstruction_error: 0.4473 - scale_error: 0.9885 - mean_l1_activity: 4.8917\n",
      "Epoch 7/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.4432 - cosine_similarity: 0.8677 - permanence_loss: 0.0013 - reconstruction_error: 0.4171 - scale_error: 0.9849 - mean_l1_activity: 6.1264\n",
      "Epoch 8/128\n",
      "125/125 [==============================] - 43s 346ms/step - loss: 0.4196 - cosine_similarity: 0.8800 - permanence_loss: 0.0021 - reconstruction_error: 0.3926 - scale_error: 0.9819 - mean_l1_activity: 6.9117\n",
      "Epoch 9/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.4040 - cosine_similarity: 0.8856 - permanence_loss: 0.0027 - reconstruction_error: 0.3765 - scale_error: 0.9804 - mean_l1_activity: 7.3739\n",
      "Epoch 10/128\n",
      "125/125 [==============================] - 39s 316ms/step - loss: 0.3959 - cosine_similarity: 0.8878 - permanence_loss: 0.0031 - reconstruction_error: 0.3680 - scale_error: 0.9794 - mean_l1_activity: 7.7045\n",
      "Epoch 11/128\n",
      "125/125 [==============================] - 43s 342ms/step - loss: 0.3875 - cosine_similarity: 0.8902 - permanence_loss: 0.0034 - reconstruction_error: 0.3592 - scale_error: 0.9788 - mean_l1_activity: 8.0467\n",
      "Epoch 12/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.3780 - cosine_similarity: 0.8930 - permanence_loss: 0.0036 - reconstruction_error: 0.3495 - scale_error: 0.9790 - mean_l1_activity: 8.2544\n",
      "Epoch 13/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.3701 - cosine_similarity: 0.8951 - permanence_loss: 0.0039 - reconstruction_error: 0.3410 - scale_error: 0.9789 - mean_l1_activity: 8.7056\n",
      "Epoch 14/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.3607 - cosine_similarity: 0.8979 - permanence_loss: 0.0043 - reconstruction_error: 0.3310 - scale_error: 0.9791 - mean_l1_activity: 9.2825\n",
      "Epoch 15/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.3535 - cosine_similarity: 0.9003 - permanence_loss: 0.0047 - reconstruction_error: 0.3233 - scale_error: 0.9790 - mean_l1_activity: 9.7222\n",
      "Epoch 16/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.3445 - cosine_similarity: 0.9028 - permanence_loss: 0.0050 - reconstruction_error: 0.3138 - scale_error: 0.9788 - mean_l1_activity: 10.1114\n",
      "Epoch 17/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.3382 - cosine_similarity: 0.9043 - permanence_loss: 0.0054 - reconstruction_error: 0.3072 - scale_error: 0.9784 - mean_l1_activity: 10.4020\n",
      "Epoch 18/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.3331 - cosine_similarity: 0.9059 - permanence_loss: 0.0059 - reconstruction_error: 0.3017 - scale_error: 0.9776 - mean_l1_activity: 10.6449\n",
      "Epoch 19/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.3268 - cosine_similarity: 0.9070 - permanence_loss: 0.0062 - reconstruction_error: 0.2953 - scale_error: 0.9773 - mean_l1_activity: 10.7068\n",
      "Epoch 20/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.3199 - cosine_similarity: 0.9088 - permanence_loss: 0.0069 - reconstruction_error: 0.2880 - scale_error: 0.9766 - mean_l1_activity: 11.0220\n",
      "Epoch 21/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.3147 - cosine_similarity: 0.9099 - permanence_loss: 0.0073 - reconstruction_error: 0.2826 - scale_error: 0.9759 - mean_l1_activity: 11.1372\n",
      "Epoch 22/128\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.3096 - cosine_similarity: 0.9112 - permanence_loss: 0.0078 - reconstruction_error: 0.2772 - scale_error: 0.9753 - mean_l1_activity: 11.2904\n",
      "Epoch 23/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.3065 - cosine_similarity: 0.9124 - permanence_loss: 0.0085 - reconstruction_error: 0.2738 - scale_error: 0.9747 - mean_l1_activity: 11.5312\n",
      "Epoch 24/128\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.3008 - cosine_similarity: 0.9138 - permanence_loss: 0.0092 - reconstruction_error: 0.2678 - scale_error: 0.9739 - mean_l1_activity: 11.7087\n",
      "Epoch 25/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.2941 - cosine_similarity: 0.9153 - permanence_loss: 0.0097 - reconstruction_error: 0.2609 - scale_error: 0.9734 - mean_l1_activity: 11.8476\n",
      "Epoch 26/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.2907 - cosine_similarity: 0.9163 - permanence_loss: 0.0103 - reconstruction_error: 0.2573 - scale_error: 0.9727 - mean_l1_activity: 11.9349\n",
      "Epoch 27/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.2849 - cosine_similarity: 0.9176 - permanence_loss: 0.0112 - reconstruction_error: 0.2510 - scale_error: 0.9718 - mean_l1_activity: 12.1938\n",
      "Epoch 28/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.2820 - cosine_similarity: 0.9186 - permanence_loss: 0.0121 - reconstruction_error: 0.2480 - scale_error: 0.9709 - mean_l1_activity: 12.2394\n",
      "Epoch 29/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.2770 - cosine_similarity: 0.9197 - permanence_loss: 0.0127 - reconstruction_error: 0.2427 - scale_error: 0.9700 - mean_l1_activity: 12.3103\n",
      "Epoch 30/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.2738 - cosine_similarity: 0.9206 - permanence_loss: 0.0137 - reconstruction_error: 0.2392 - scale_error: 0.9693 - mean_l1_activity: 12.5103\n",
      "Epoch 31/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.2714 - cosine_similarity: 0.9212 - permanence_loss: 0.0142 - reconstruction_error: 0.2368 - scale_error: 0.9687 - mean_l1_activity: 12.4066\n",
      "Epoch 32/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.2665 - cosine_similarity: 0.9225 - permanence_loss: 0.0152 - reconstruction_error: 0.2316 - scale_error: 0.9674 - mean_l1_activity: 12.5328\n",
      "Epoch 33/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.2655 - cosine_similarity: 0.9230 - permanence_loss: 0.0161 - reconstruction_error: 0.2303 - scale_error: 0.9661 - mean_l1_activity: 12.6894\n",
      "Epoch 34/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.2616 - cosine_similarity: 0.9239 - permanence_loss: 0.0174 - reconstruction_error: 0.2261 - scale_error: 0.9651 - mean_l1_activity: 12.7188\n",
      "Epoch 35/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.2599 - cosine_similarity: 0.9246 - permanence_loss: 0.0181 - reconstruction_error: 0.2242 - scale_error: 0.9642 - mean_l1_activity: 12.8455\n",
      "Epoch 36/128\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.2549 - cosine_similarity: 0.9258 - permanence_loss: 0.0195 - reconstruction_error: 0.2187 - scale_error: 0.9628 - mean_l1_activity: 13.0607\n",
      "Epoch 37/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.2506 - cosine_similarity: 0.9266 - permanence_loss: 0.0207 - reconstruction_error: 0.2140 - scale_error: 0.9613 - mean_l1_activity: 13.1853\n",
      "Epoch 38/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 34s 270ms/step - loss: 0.2506 - cosine_similarity: 0.9270 - permanence_loss: 0.0220 - reconstruction_error: 0.2137 - scale_error: 0.9596 - mean_l1_activity: 13.3618\n",
      "Epoch 39/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.2451 - cosine_similarity: 0.9282 - permanence_loss: 0.0226 - reconstruction_error: 0.2082 - scale_error: 0.9586 - mean_l1_activity: 13.2054\n",
      "Epoch 40/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.2435 - cosine_similarity: 0.9287 - permanence_loss: 0.0242 - reconstruction_error: 0.2061 - scale_error: 0.9569 - mean_l1_activity: 13.4880\n",
      "Epoch 41/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.2386 - cosine_similarity: 0.9299 - permanence_loss: 0.0254 - reconstruction_error: 0.2008 - scale_error: 0.9553 - mean_l1_activity: 13.5972\n",
      "Epoch 42/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.2397 - cosine_similarity: 0.9302 - permanence_loss: 0.0273 - reconstruction_error: 0.2014 - scale_error: 0.9530 - mean_l1_activity: 13.7835\n",
      "Epoch 43/128\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.2374 - cosine_similarity: 0.9309 - permanence_loss: 0.0293 - reconstruction_error: 0.1987 - scale_error: 0.9513 - mean_l1_activity: 13.8037\n",
      "Epoch 44/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.2350 - cosine_similarity: 0.9314 - permanence_loss: 0.0298 - reconstruction_error: 0.1963 - scale_error: 0.9499 - mean_l1_activity: 13.6658\n",
      "Epoch 45/128\n",
      "125/125 [==============================] - 46s 369ms/step - loss: 0.2354 - cosine_similarity: 0.9320 - permanence_loss: 0.0322 - reconstruction_error: 0.1960 - scale_error: 0.9473 - mean_l1_activity: 14.0112\n",
      "Epoch 46/128\n",
      "125/125 [==============================] - 47s 379ms/step - loss: 0.2282 - cosine_similarity: 0.9335 - permanence_loss: 0.0327 - reconstruction_error: 0.1889 - scale_error: 0.9461 - mean_l1_activity: 13.8559\n",
      "Epoch 47/128\n",
      "125/125 [==============================] - 47s 379ms/step - loss: 0.2254 - cosine_similarity: 0.9340 - permanence_loss: 0.0346 - reconstruction_error: 0.1856 - scale_error: 0.9441 - mean_l1_activity: 14.0082\n",
      "Epoch 48/128\n",
      "125/125 [==============================] - 47s 379ms/step - loss: 0.2253 - cosine_similarity: 0.9344 - permanence_loss: 0.0355 - reconstruction_error: 0.1854 - scale_error: 0.9421 - mean_l1_activity: 13.9331\n",
      "Epoch 49/128\n",
      "125/125 [==============================] - 47s 379ms/step - loss: 0.2237 - cosine_similarity: 0.9351 - permanence_loss: 0.0382 - reconstruction_error: 0.1831 - scale_error: 0.9393 - mean_l1_activity: 14.2012\n",
      "Epoch 50/128\n",
      "125/125 [==============================] - 47s 379ms/step - loss: 0.2228 - cosine_similarity: 0.9355 - permanence_loss: 0.0409 - reconstruction_error: 0.1814 - scale_error: 0.9363 - mean_l1_activity: 14.4458\n",
      "Epoch 51/128\n",
      "125/125 [==============================] - 47s 378ms/step - loss: 0.2234 - cosine_similarity: 0.9356 - permanence_loss: 0.0412 - reconstruction_error: 0.1822 - scale_error: 0.9349 - mean_l1_activity: 14.2657\n",
      "Epoch 52/128\n",
      "125/125 [==============================] - 47s 379ms/step - loss: 0.2184 - cosine_similarity: 0.9368 - permanence_loss: 0.0424 - reconstruction_error: 0.1770 - scale_error: 0.9330 - mean_l1_activity: 14.2618\n",
      "Epoch 53/128\n",
      "125/125 [==============================] - 47s 378ms/step - loss: 0.2198 - cosine_similarity: 0.9368 - permanence_loss: 0.0442 - reconstruction_error: 0.1779 - scale_error: 0.9303 - mean_l1_activity: 14.4410\n",
      "Epoch 54/128\n",
      "125/125 [==============================] - 48s 381ms/step - loss: 0.2154 - cosine_similarity: 0.9379 - permanence_loss: 0.0471 - reconstruction_error: 0.1729 - scale_error: 0.9274 - mean_l1_activity: 14.5093\n",
      "Epoch 55/128\n",
      "125/125 [==============================] - 48s 382ms/step - loss: 0.2194 - cosine_similarity: 0.9377 - permanence_loss: 0.0506 - reconstruction_error: 0.1760 - scale_error: 0.9230 - mean_l1_activity: 14.7682\n",
      "Epoch 56/128\n",
      "125/125 [==============================] - 48s 381ms/step - loss: 0.2101 - cosine_similarity: 0.9391 - permanence_loss: 0.0511 - reconstruction_error: 0.1670 - scale_error: 0.9221 - mean_l1_activity: 14.5130\n",
      "Epoch 57/128\n",
      "125/125 [==============================] - 48s 381ms/step - loss: 0.2117 - cosine_similarity: 0.9391 - permanence_loss: 0.0540 - reconstruction_error: 0.1679 - scale_error: 0.9182 - mean_l1_activity: 14.6656\n",
      "Epoch 58/128\n",
      "125/125 [==============================] - 48s 382ms/step - loss: 0.2114 - cosine_similarity: 0.9396 - permanence_loss: 0.0540 - reconstruction_error: 0.1678 - scale_error: 0.9166 - mean_l1_activity: 14.4974\n",
      "Epoch 59/128\n",
      "125/125 [==============================] - 48s 382ms/step - loss: 0.2080 - cosine_similarity: 0.9403 - permanence_loss: 0.0560 - reconstruction_error: 0.1640 - scale_error: 0.9137 - mean_l1_activity: 14.5584\n",
      "Epoch 60/128\n",
      "125/125 [==============================] - 48s 382ms/step - loss: 0.2091 - cosine_similarity: 0.9407 - permanence_loss: 0.0597 - reconstruction_error: 0.1641 - scale_error: 0.9099 - mean_l1_activity: 14.8559\n",
      "Epoch 61/128\n",
      "125/125 [==============================] - 48s 383ms/step - loss: 0.2131 - cosine_similarity: 0.9400 - permanence_loss: 0.0615 - reconstruction_error: 0.1679 - scale_error: 0.9082 - mean_l1_activity: 14.7323\n",
      "Epoch 62/128\n",
      "125/125 [==============================] - 48s 383ms/step - loss: 0.2070 - cosine_similarity: 0.9413 - permanence_loss: 0.0637 - reconstruction_error: 0.1615 - scale_error: 0.9051 - mean_l1_activity: 14.6187\n",
      "Epoch 63/128\n",
      "125/125 [==============================] - 48s 384ms/step - loss: 0.2060 - cosine_similarity: 0.9416 - permanence_loss: 0.0643 - reconstruction_error: 0.1605 - scale_error: 0.9035 - mean_l1_activity: 14.5143\n",
      "Epoch 64/128\n",
      "125/125 [==============================] - 554s 4s/step - loss: 0.2044 - cosine_similarity: 0.9421 - permanence_loss: 0.0673 - reconstruction_error: 0.1583 - scale_error: 0.8998 - mean_l1_activity: 14.6027\n",
      "Epoch 65/128\n",
      "125/125 [==============================] - 43s 345ms/step - loss: 0.2031 - cosine_similarity: 0.9425 - permanence_loss: 0.0683 - reconstruction_error: 0.1568 - scale_error: 0.8978 - mean_l1_activity: 14.6779\n",
      "Epoch 66/128\n",
      "125/125 [==============================] - 43s 347ms/step - loss: 0.2014 - cosine_similarity: 0.9428 - permanence_loss: 0.0694 - reconstruction_error: 0.1551 - scale_error: 0.8953 - mean_l1_activity: 14.5531\n",
      "Epoch 67/128\n",
      "125/125 [==============================] - 43s 343ms/step - loss: 0.2012 - cosine_similarity: 0.9433 - permanence_loss: 0.0715 - reconstruction_error: 0.1543 - scale_error: 0.8933 - mean_l1_activity: 14.7009\n",
      "Epoch 68/128\n",
      "125/125 [==============================] - 49s 389ms/step - loss: 0.2011 - cosine_similarity: 0.9437 - permanence_loss: 0.0751 - reconstruction_error: 0.1535 - scale_error: 0.8892 - mean_l1_activity: 14.8012\n",
      "Epoch 69/128\n",
      "125/125 [==============================] - 48s 382ms/step - loss: 0.1961 - cosine_similarity: 0.9447 - permanence_loss: 0.0765 - reconstruction_error: 0.1484 - scale_error: 0.8874 - mean_l1_activity: 14.6702\n",
      "Epoch 70/128\n",
      "125/125 [==============================] - 47s 374ms/step - loss: 0.2005 - cosine_similarity: 0.9443 - permanence_loss: 0.0793 - reconstruction_error: 0.1522 - scale_error: 0.8844 - mean_l1_activity: 14.7552\n",
      "Epoch 71/128\n",
      "125/125 [==============================] - 43s 346ms/step - loss: 0.1983 - cosine_similarity: 0.9449 - permanence_loss: 0.0822 - reconstruction_error: 0.1495 - scale_error: 0.8819 - mean_l1_activity: 14.7340\n",
      "Epoch 72/128\n",
      "125/125 [==============================] - 41s 330ms/step - loss: 0.1971 - cosine_similarity: 0.9453 - permanence_loss: 0.0846 - reconstruction_error: 0.1478 - scale_error: 0.8801 - mean_l1_activity: 14.8032\n",
      "Epoch 73/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1943 - cosine_similarity: 0.9457 - permanence_loss: 0.0848 - reconstruction_error: 0.1451 - scale_error: 0.8784 - mean_l1_activity: 14.6582\n",
      "Epoch 74/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1962 - cosine_similarity: 0.9457 - permanence_loss: 0.0850 - reconstruction_error: 0.1471 - scale_error: 0.8769 - mean_l1_activity: 14.5670\n",
      "Epoch 75/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 34s 271ms/step - loss: 0.1952 - cosine_similarity: 0.9460 - permanence_loss: 0.0887 - reconstruction_error: 0.1454 - scale_error: 0.8738 - mean_l1_activity: 14.6109\n",
      "Epoch 76/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1993 - cosine_similarity: 0.9458 - permanence_loss: 0.0925 - reconstruction_error: 0.1488 - scale_error: 0.8709 - mean_l1_activity: 14.5955\n",
      "Epoch 77/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.1990 - cosine_similarity: 0.9458 - permanence_loss: 0.0968 - reconstruction_error: 0.1477 - scale_error: 0.8693 - mean_l1_activity: 14.5447\n",
      "Epoch 78/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1950 - cosine_similarity: 0.9467 - permanence_loss: 0.0957 - reconstruction_error: 0.1440 - scale_error: 0.8693 - mean_l1_activity: 14.5706\n",
      "Epoch 79/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.1921 - cosine_similarity: 0.9473 - permanence_loss: 0.0957 - reconstruction_error: 0.1413 - scale_error: 0.8683 - mean_l1_activity: 14.3486\n",
      "Epoch 80/128\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 0.1953 - cosine_similarity: 0.9472 - permanence_loss: 0.1016 - reconstruction_error: 0.1431 - scale_error: 0.8657 - mean_l1_activity: 14.5666\n",
      "Epoch 81/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.1903 - cosine_similarity: 0.9480 - permanence_loss: 0.0995 - reconstruction_error: 0.1389 - scale_error: 0.8637 - mean_l1_activity: 14.2489\n",
      "Epoch 82/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1900 - cosine_similarity: 0.9483 - permanence_loss: 0.1022 - reconstruction_error: 0.1380 - scale_error: 0.8627 - mean_l1_activity: 14.3262\n",
      "Epoch 83/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1878 - cosine_similarity: 0.9485 - permanence_loss: 0.1004 - reconstruction_error: 0.1364 - scale_error: 0.8622 - mean_l1_activity: 14.0511\n",
      "Epoch 84/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.1874 - cosine_similarity: 0.9487 - permanence_loss: 0.1033 - reconstruction_error: 0.1355 - scale_error: 0.8607 - mean_l1_activity: 14.0786\n",
      "Epoch 85/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1933 - cosine_similarity: 0.9477 - permanence_loss: 0.1076 - reconstruction_error: 0.1404 - scale_error: 0.8575 - mean_l1_activity: 14.2156\n",
      "Epoch 86/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1932 - cosine_similarity: 0.9482 - permanence_loss: 0.1104 - reconstruction_error: 0.1398 - scale_error: 0.8568 - mean_l1_activity: 14.1236\n",
      "Epoch 87/128\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.1890 - cosine_similarity: 0.9491 - permanence_loss: 0.1107 - reconstruction_error: 0.1356 - scale_error: 0.8550 - mean_l1_activity: 14.1966\n",
      "Epoch 88/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1897 - cosine_similarity: 0.9489 - permanence_loss: 0.1103 - reconstruction_error: 0.1366 - scale_error: 0.8549 - mean_l1_activity: 13.8859\n",
      "Epoch 89/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1847 - cosine_similarity: 0.9500 - permanence_loss: 0.1111 - reconstruction_error: 0.1317 - scale_error: 0.8537 - mean_l1_activity: 13.7521\n",
      "Epoch 90/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.2002 - cosine_similarity: 0.9471 - permanence_loss: 0.1181 - reconstruction_error: 0.1452 - scale_error: 0.8511 - mean_l1_activity: 14.3299\n",
      "Epoch 91/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1916 - cosine_similarity: 0.9490 - permanence_loss: 0.1160 - reconstruction_error: 0.1374 - scale_error: 0.8489 - mean_l1_activity: 13.9375\n",
      "Epoch 92/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.1874 - cosine_similarity: 0.9499 - permanence_loss: 0.1176 - reconstruction_error: 0.1330 - scale_error: 0.8463 - mean_l1_activity: 13.9633\n",
      "Epoch 93/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1849 - cosine_similarity: 0.9505 - permanence_loss: 0.1175 - reconstruction_error: 0.1307 - scale_error: 0.8463 - mean_l1_activity: 13.7429\n",
      "Epoch 94/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.1885 - cosine_similarity: 0.9501 - permanence_loss: 0.1191 - reconstruction_error: 0.1340 - scale_error: 0.8446 - mean_l1_activity: 13.7922\n",
      "Epoch 95/128\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 0.1836 - cosine_similarity: 0.9510 - permanence_loss: 0.1212 - reconstruction_error: 0.1288 - scale_error: 0.8434 - mean_l1_activity: 13.7680\n",
      "Epoch 96/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1856 - cosine_similarity: 0.9507 - permanence_loss: 0.1221 - reconstruction_error: 0.1307 - scale_error: 0.8429 - mean_l1_activity: 13.5554\n",
      "Epoch 97/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1832 - cosine_similarity: 0.9512 - permanence_loss: 0.1239 - reconstruction_error: 0.1279 - scale_error: 0.8421 - mean_l1_activity: 13.6723\n",
      "Epoch 98/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.1801 - cosine_similarity: 0.9519 - permanence_loss: 0.1253 - reconstruction_error: 0.1246 - scale_error: 0.8412 - mean_l1_activity: 13.6517\n",
      "Epoch 99/128\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.1811 - cosine_similarity: 0.9520 - permanence_loss: 0.1283 - reconstruction_error: 0.1250 - scale_error: 0.8398 - mean_l1_activity: 13.6308\n",
      "Epoch 100/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1937 - cosine_similarity: 0.9495 - permanence_loss: 0.1269 - reconstruction_error: 0.1381 - scale_error: 0.8400 - mean_l1_activity: 13.4646\n",
      "Epoch 101/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.1869 - cosine_similarity: 0.9506 - permanence_loss: 0.1216 - reconstruction_error: 0.1329 - scale_error: 0.8373 - mean_l1_activity: 12.8603\n",
      "Epoch 102/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1803 - cosine_similarity: 0.9519 - permanence_loss: 0.1250 - reconstruction_error: 0.1254 - scale_error: 0.8378 - mean_l1_activity: 13.1362\n",
      "Epoch 103/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1816 - cosine_similarity: 0.9519 - permanence_loss: 0.1263 - reconstruction_error: 0.1265 - scale_error: 0.8355 - mean_l1_activity: 13.0707\n",
      "Epoch 104/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1896 - cosine_similarity: 0.9501 - permanence_loss: 0.1289 - reconstruction_error: 0.1344 - scale_error: 0.8352 - mean_l1_activity: 12.8126\n",
      "Epoch 105/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.1809 - cosine_similarity: 0.9514 - permanence_loss: 0.1202 - reconstruction_error: 0.1279 - scale_error: 0.8320 - mean_l1_activity: 12.3258\n",
      "Epoch 106/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1769 - cosine_similarity: 0.9519 - permanence_loss: 0.1156 - reconstruction_error: 0.1252 - scale_error: 0.8310 - mean_l1_activity: 11.9552\n",
      "Epoch 107/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1764 - cosine_similarity: 0.9523 - permanence_loss: 0.1146 - reconstruction_error: 0.1250 - scale_error: 0.8304 - mean_l1_activity: 11.9303\n",
      "Epoch 108/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1793 - cosine_similarity: 0.9520 - permanence_loss: 0.1173 - reconstruction_error: 0.1273 - scale_error: 0.8253 - mean_l1_activity: 12.0304\n",
      "Epoch 109/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1755 - cosine_similarity: 0.9528 - permanence_loss: 0.1179 - reconstruction_error: 0.1235 - scale_error: 0.8244 - mean_l1_activity: 11.8966\n",
      "Epoch 110/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.1774 - cosine_similarity: 0.9525 - permanence_loss: 0.1176 - reconstruction_error: 0.1257 - scale_error: 0.8207 - mean_l1_activity: 11.7548\n",
      "Epoch 111/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1719 - cosine_similarity: 0.9534 - permanence_loss: 0.1163 - reconstruction_error: 0.1207 - scale_error: 0.8200 - mean_l1_activity: 11.5419\n",
      "Epoch 112/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1711 - cosine_similarity: 0.9535 - permanence_loss: 0.1164 - reconstruction_error: 0.1200 - scale_error: 0.8176 - mean_l1_activity: 11.5223\n",
      "Epoch 113/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1717 - cosine_similarity: 0.9535 - permanence_loss: 0.1174 - reconstruction_error: 0.1204 - scale_error: 0.8177 - mean_l1_activity: 11.5177\n",
      "Epoch 114/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1707 - cosine_similarity: 0.9538 - permanence_loss: 0.1174 - reconstruction_error: 0.1195 - scale_error: 0.8148 - mean_l1_activity: 11.4143\n",
      "Epoch 115/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1680 - cosine_similarity: 0.9544 - permanence_loss: 0.1163 - reconstruction_error: 0.1171 - scale_error: 0.8139 - mean_l1_activity: 11.3144\n",
      "Epoch 116/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1706 - cosine_similarity: 0.9539 - permanence_loss: 0.1171 - reconstruction_error: 0.1197 - scale_error: 0.8087 - mean_l1_activity: 11.3456\n",
      "Epoch 117/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1747 - cosine_similarity: 0.9533 - permanence_loss: 0.1196 - reconstruction_error: 0.1233 - scale_error: 0.8071 - mean_l1_activity: 11.3845\n",
      "Epoch 118/128\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.1686 - cosine_similarity: 0.9545 - permanence_loss: 0.1184 - reconstruction_error: 0.1176 - scale_error: 0.8053 - mean_l1_activity: 11.2100\n",
      "Epoch 119/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1702 - cosine_similarity: 0.9542 - permanence_loss: 0.1185 - reconstruction_error: 0.1192 - scale_error: 0.8048 - mean_l1_activity: 11.1895\n",
      "Epoch 120/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1798 - cosine_similarity: 0.9526 - permanence_loss: 0.1235 - reconstruction_error: 0.1277 - scale_error: 0.8056 - mean_l1_activity: 11.2885\n",
      "Epoch 121/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.1746 - cosine_similarity: 0.9536 - permanence_loss: 0.1243 - reconstruction_error: 0.1224 - scale_error: 0.8017 - mean_l1_activity: 11.3094\n",
      "Epoch 122/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1681 - cosine_similarity: 0.9547 - permanence_loss: 0.1215 - reconstruction_error: 0.1167 - scale_error: 0.7986 - mean_l1_activity: 11.1586\n",
      "Epoch 123/128\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 0.1687 - cosine_similarity: 0.9547 - permanence_loss: 0.1237 - reconstruction_error: 0.1168 - scale_error: 0.7994 - mean_l1_activity: 11.2060\n",
      "Epoch 124/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1871 - cosine_similarity: 0.9517 - permanence_loss: 0.1397 - reconstruction_error: 0.1313 - scale_error: 0.8056 - mean_l1_activity: 11.7761\n",
      "Epoch 125/128\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1739 - cosine_similarity: 0.9537 - permanence_loss: 0.1234 - reconstruction_error: 0.1222 - scale_error: 0.7958 - mean_l1_activity: 11.0959\n",
      "Epoch 126/128\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.1663 - cosine_similarity: 0.9552 - permanence_loss: 0.1225 - reconstruction_error: 0.1148 - scale_error: 0.7924 - mean_l1_activity: 11.1088\n",
      "Epoch 127/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1655 - cosine_similarity: 0.9555 - permanence_loss: 0.1242 - reconstruction_error: 0.1137 - scale_error: 0.7913 - mean_l1_activity: 11.1565\n",
      "Epoch 128/128\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.1681 - cosine_similarity: 0.9553 - permanence_loss: 0.1277 - reconstruction_error: 0.1155 - scale_error: 0.7921 - mean_l1_activity: 11.2210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'current': 1024547328, 'peak': 5084171264}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = \"ptolemynet_logs/fit_layer1/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_images=False)\n",
    "profiling_tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, write_images=False, profile_batch=(10, 20))\n",
    "animation_callbacks = [\n",
    "    AnimateReconstruction(\n",
    "        [l1],\n",
    "        tf.concat([X[i] for i in range(num_inputs_per_run)], axis=0),\n",
    "        log_dir + \"/animation_x1.gif\",\n",
    "    ),\n",
    "    AnimateReconstruction(\n",
    "        [l1],\n",
    "        tf.concat([X2[i] for i in range(num_inputs_per_run)], axis=0),\n",
    "        log_dir + \"/animation_x2.gif\",\n",
    "    )\n",
    "]\n",
    "\n",
    "tf.config.experimental.reset_memory_stats('GPU:0')\n",
    "X_1 = np.concatenate((X, X2))\n",
    "history = reconstruction_model.fit([X_1], X_1, epochs=128, batch_size=64, callbacks=[tensorboard_callback] + animation_callbacks)\n",
    "#history = reconstruction_model.fit([X_1], X_1, epochs=1, batch_size=64, callbacks=[profiling_tensorboard_callback])\n",
    "#history = reconstruction_model.fit([X_1], X_1, epochs=1, batch_size=64)\n",
    "tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruction_model.save('ptolemy_model_saves/2021-11-18_PtolemyNet31_mnist_layer1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "167c2c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context sharpness [5.1764    5.1137137 4.6709857 4.924753  4.3780775 4.995231  5.2619376\n",
      " 5.2495866 5.235913  4.976365  4.13174   4.761254  4.505045  4.8377123\n",
      " 4.6196523 4.9322906 4.448754  4.4632177 4.955061  5.198921  4.986793\n",
      " 4.522601  4.7681103 5.174151  4.8074617 4.702092  5.1240892 4.786551\n",
      " 4.2504034 5.0246444 4.5216784 4.921919 ]\n",
      "selection_sharpness [2.3791265]\n",
      "suppression_sharpness [-0.3500237]\n",
      "stickiness [0.05221336]\n"
     ]
    }
   ],
   "source": [
    "print('context sharpness', l1.columns[0].context_aggregator.context_sharpness.numpy())\n",
    "print('selection_sharpness', l1.columns[0].column_selector.selection_sharpness.numpy())\n",
    "print('suppression_sharpness', l1.columns[0].column_selector.suppression_sharpness.numpy())\n",
    "print('stickiness', l1.columns[0].column_selector.stickiness.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7cc34c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAyADIBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiu0+G2kaZq+qaqNVsI72K208zRxSSSIu/zokySjKejt3ru/wDhGvCn/Qq2P/gTd/8Ax6s3xL4a8NxeDdavLPQba0urWCOSKaKe4YgmeJDw8jA/K7dq8iooor0D4T/8hPXv+wV/7c29d/Wf4m/5ELxJ/wBekP8A6VQV4hRRRW54X8UXPhW9ubm2tLS6+025t5I7oOV270fI2MpzlF7+tdJ/wti+/wChd0P8rn/4/VPV/iTf6vol7pR0fSrWK8RUkkgE+/CurgDfKw6oO1cXRRXvdpaafBpGkomjaM27TLN2aTTLd2ZmgjZiWZCSSSTknvUnl2P/AEBdD/8ABRbf/G68s+JNvb23jm7jtraC3iNvav5UESxoC1tEzEKoAGSSeB3rk6KKK9/j/wCQZpH/AGCrH/0mjorzD4n/API+3X/XpZf+ksVcfRRRX2n4L0nTbnwF4bmuNPtJZW0q13PJCrMf3KDkkVuf2Do//QJsf/AdP8K+TvjTGkXxa1qONFREFuqqowABbx4AFcDRRRX/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAAAAAA7VNdtAAAAaElEQVR4Ae2UwQrAIAxDq+zD/fPZ4aWhUGKviydjUtEn1UzjvwQGXP0F5QLt4z4YWqU85sQMo1TCUIoZEYs0mHmDGDYE1S/MSZSJBIZdg/VPacUtkkTzU43XV0nGWK+IWM0nuw1ijX7ZMKAGRahuxacAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[570]\n",
    "#x = (X2[518] + X2[1]) / 2\n",
    "x = X2[518]\n",
    "x_img = vecs_to_img(x[0])\n",
    "keras.utils.array_to_img(x_img, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c36a1df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "-8.038093\n",
      "11.472177\n",
      "norm of x 1.0000000033069503\n",
      "norm of y 0.51894087\n",
      "0.0\n",
      "253.93283\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAyADIBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APCdsu4sWbP86khQvGNpxg+tKbcngg+1Ma32qc8fhULM2cA9PekV3BHzHOavpgIuY0zjulQF8J0Ge+BXcfCXRdP1zX9Qi1O2S5t4rJpRGzsvzB0AI2kdmPFegHwn4UWY/wDEnUrnkGeX/wCKrF8Y+DPDlr4Nv7/TrNoLyHYUbzpGzl1B4LEdCa8YPGRjmhRubpWysEYRcyNnHPFZcmQ5HHQV6D8HZ3j8Q38Q4Elk4bI/20I/lXpc0LeYay/GIaLwBqrldw2x8EZxmVa8HEZdm4zTQpEgAHfoKul+eZHz3+YVXcARgk5cfhW1oXiW40OVprNUV2GGJXO6uk/4W1qSLxp1q7f3n3f0rP8AEPxH1TX9Ik057S1treUL5nlglmKncOSeOQO3auVDqFLYGcdRVYbmdcHJNay25ZQSoyRmvfINH0ZdA0x202yldrWEs7wKxY7AMk45OKkg0jw+6mP+xbBd2RlbdMjP4V4b400eHQvGOoabAD5UbI6A/wB1kDj9GrAkOFK7ecUxWyuMfrRHw6+v1rXFztGMDjjtXv1tHM+g6YxBObSHn/gAqS1t5fOA2tya8c+LG9fiNfuUxiK3Bwf+mEdcWz72UAbj/KmNE4ydmB7VLBC5kUFcZOM100elQ+Wu4W+7Azm4jHP/AH1X0N8NJH1PRp5dQdruTNn89wfMPzWNuzcnPVmYn1JJ712gs7UKhFtDnH9we9fIvxFkd/iHr+52bbdsoyc4AAAH0AAH4VhW6rjO0Zz1xVhgPKAx3pkJ23SbeMqen1Aqj9suV+VbmYAcABzxX//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = reconstruction_model.predict(tf.reshape(x, (1, 3, x_size * y_size, 1, 2)))\n",
    "print(tf.reduce_min(y[0][0]).numpy())\n",
    "print(tf.reduce_max(y[0][0]).numpy())\n",
    "print('norm of x', tf.reduce_mean(tf.norm(x[0], axis=-1)).numpy())\n",
    "print('norm of y', tf.reduce_mean(tf.norm(y[0][0], axis=-1)).numpy())\n",
    "y_img = vecs_to_img(tf.squeeze(y[0][0]).numpy())\n",
    "print(tf.reduce_min(y_img).numpy())\n",
    "print(tf.reduce_max(y_img).numpy())\n",
    "keras.utils.array_to_img(y_img, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e083f9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (1, 8, 40)\n",
      "min -3.167332\n",
      "max 1.5488524\n",
      "norm 5.370476\n",
      "max of diff 1.2060113\n",
      "norm of diff 1.7109392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7669983d6840>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "us = reconstruction_model.get_layer(index=1)(x.reshape((1, 3, x_size * y_size, 1, 2)), training=False)[0]\n",
    "us = tf.cast(us, tf.float32)\n",
    "one_us = us[0]\n",
    "usd = one_us[0] - one_us[1]\n",
    "one_us = one_us[0]\n",
    "print('shape', one_us.shape)\n",
    "print('min', tf.reduce_min(one_us).numpy())\n",
    "print('max', tf.reduce_max(one_us).numpy())\n",
    "print('norm', tf.norm(one_us).numpy())\n",
    "print('max of diff', tf.reduce_max(tf.abs(usd)).numpy())\n",
    "print('norm of diff', tf.norm(usd).numpy())\n",
    "plt.matshow(one_us.numpy().T)\n",
    "plt.matshow(usd.numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f62313d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of diff 11.088212\n",
      "(1, 3, 2500, 1, 2)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAyADIBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APCSJBMcsxbIzz1pw3pjn5VPJBqdYEkiBye4Bz/SqTLgkHjnpTiz8AHp701XcMPnOc1oJ9xcxoTjnKVUldjKxKrnjjGM0hKOu3aB05FCSNGB0Ix1x61GxZj70uRsOAP8KRFLnpjvWysEYUAu2cc1mrCZ59mQuQO/HTtWpHo9zLCGEEg+XBYg8ZPP4f8A16z57R7aYoVyQ2GUdf1qBUDFgityM+uB/X/61T/Z8xYI5AHA4qttKuAPyFXC/Jy7Z78ioGxGqtk716fnW5D4lu/I8sEYxjp1rJvL5riRiyqGJyflxnP/ANeooHRSWLncAMe/r9KtGVQmcDIH3s/0/Cs7LPIDkkmtYW7MoJXkjNe/RaJoa6Fpsp0uxkke1hLSNApZzsABJxzx3q3p9ro9u4MWl2cbcgMkKgjPXtXj/wAVbO0h8aSx2tukX7mNiFXAPHXH0/lXBONjbeM8HmpC4ZR6Y9ajRQGXHX61reeV4x04617/AG6StoWmEgnNpD/6AKktbeQygBW57V458U3kj+Id/vBwsUC9On7pD/X2/rXGO/mOu1d1RyRSKSdmBntU1rGzyqCDgnGa7GLR7MwoXW3LbRnN3AOfoXzXvHw5kfU9Jnlv3a7k22h3znzD81hbseT6szMfck967g2VqrjFtCPvdIx6V8n/ABXJ/wCFlayO3mgfkoA/QAfhXP2UaFASi53dcVbmRduNox6YrS0KGIXasI0DeXJztGa5M313k/6VP/38Nf/Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=50x50>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "us1 = reconstruction_model.get_layer(index=1)(tf.reshape(X2[518], (1, 3, x_size * y_size, 1, 2)), training=False)[0]\n",
    "us2 = reconstruction_model.get_layer(index=1)(tf.reshape(X[4], (1, 3, x_size * y_size, 1, 2)), training=False)[0]\n",
    "us1 = tf.cast(us1, tf.float32)\n",
    "us2 = tf.cast(us2, tf.float32)\n",
    "print('norm of diff', tf.norm(us2-us1).numpy())\n",
    "us_mod = (us1 + 0.0 * (us2 - us1)).numpy()\n",
    "#for i in range(1, 16):\n",
    "#    us_mod[0][0][0][i] = np.zeros((40))\n",
    "#us_mod[0][0][0][0] *= 16\n",
    "plt.matshow(us_mod[0][0][0].T)\n",
    "us_mod[0][0][0][1] = np.zeros((40))\n",
    "#us_mod[0][0][0][15] = 0.2\n",
    "r = l1.reconstruct(tf.cast(us_mod, tf.float32))\n",
    "print(r.shape)\n",
    "r_one = r[0][0]\n",
    "r_img = vecs_to_img(tf.squeeze(r_one).numpy())\n",
    "keras.utils.array_to_img(r_img, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd914dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = l1.columns[0].context_aggregator.downstream_type_positions.numpy()\n",
    "le = le.reshape((x_size,y_size,-1))\n",
    "print(tf.reduce_min(le[:, :, 0]).numpy())\n",
    "print(tf.reduce_max(le[:, :, 0]).numpy())\n",
    "for d in range(0, le.shape[2]):\n",
    "    plt.matshow(le[:, :, d])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac660b6",
   "metadata": {},
   "source": [
    "# Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790ff2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_2 = upstream_state_model.predict(np.concatenate((X, X2)), batch_size=64)\n",
    "#X_2 = upstream_state_model.predict(X, batch_size=64)\n",
    "\n",
    "Y_2 = X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff471ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_2 = Input(shape=(3, 1, 8, 40))\n",
    "l2 = PtolemyLayer(\n",
    "    num_types=1,\n",
    "    num_instances_per_type=2,\n",
    "    context_fan_in=8,\n",
    "    upstream_dim=8,\n",
    "    type_position_dim=0,\n",
    "    instance_position_dim=8,\n",
    "    stride=1,\n",
    "    permanence_loss_rho=0.5,\n",
    "    initial_sharpness=3.0,\n",
    "    noise_rate=0.2,\n",
    "    add_default=False,\n",
    ")\n",
    "\n",
    "upstream_state_2, downstream_reconstruction_2 = l2(inp_2)\n",
    "\n",
    "reconstruction_model_2 = keras.models.Model(inputs=[inp_2], outputs=downstream_reconstruction_2)\n",
    "upstream_state_model_2 = keras.models.Model(inputs=[inp_2], outputs=upstream_state_2)\n",
    "\n",
    "optimizer_2 = keras.optimizers.Adam(\n",
    "    learning_rate = 0.001,\n",
    "    clipnorm=0.1,\n",
    ")\n",
    "#optimizer_2 = keras.mixed_precision.LossScaleOptimizer(optimizer_2)\n",
    "reconstruction_model_2.compile(\n",
    "    loss = zero_loss,\n",
    "    optimizer = optimizer_2,\n",
    "    metrics = ['cosine_similarity'],\n",
    ")\n",
    "reconstruction_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225f0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dir = \"ptolemynet_logs/fit_layer2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_images=False)\n",
    "animation_callbacks = [\n",
    "    AnimateReconstruction(\n",
    "        [l1, l2],\n",
    "        tf.concat([X[i] for i in range(num_inputs_per_run)], axis=0),\n",
    "        log_dir + \"/animation_x1.gif\",\n",
    "    ),\n",
    "    AnimateReconstruction(\n",
    "        [l1, l2],\n",
    "        tf.concat([X2[i] for i in range(num_inputs_per_run)], axis=0),\n",
    "        log_dir + \"/animation_x2.gif\",\n",
    "    )\n",
    "]\n",
    "\n",
    "tf.config.experimental.reset_memory_stats('GPU:0')\n",
    "history = reconstruction_model_2.fit([X_2], Y_2, epochs=128, batch_size=64, callbacks=[tensorboard_callback] + animation_callbacks)\n",
    "tf.config.experimental.get_memory_info('GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960518e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = X_2[4500]\n",
    "#x_2 = X_2[570]\n",
    "x_2_img = vecs_to_img(l1.reconstruct(tf.cast(tf.expand_dims(x_2, axis=0), tf.float32))[0][0].numpy())\n",
    "keras.utils.array_to_img(x_2_img, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2 = reconstruction_model_2.predict(tf.reshape(x_2, (1, 3, 1, 8, 40)))[0]\n",
    "y_2_img = vecs_to_img(l1.reconstruct(tf.cast(tf.expand_dims(y_2, axis=0), tf.float32))[0][0].numpy())\n",
    "keras.utils.array_to_img(y_2_img, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a37081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.matshow(tf.cast(x_2[0], tf.float32).numpy().T)\n",
    "plt.matshow(tf.cast(y_2[0], tf.float32).numpy().T)\n",
    "print(tf.norm(x_2[0]).numpy())\n",
    "print(tf.norm(y_2[0]).numpy())\n",
    "rd = tf.cast(x_2[0], tf.float32).numpy() - tf.cast(y_2[0], tf.float32).numpy()\n",
    "plt.matshow(np.abs(rd.T), vmin=0.0, vmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be140fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "us = reconstruction_model_2.get_layer(index=1)(x_2.reshape((1, 3, 1, 8, 40)), training=False)[0]\n",
    "us = tf.cast(us, tf.float32)\n",
    "one_us = us[0]\n",
    "usd = one_us[0] - one_us[1]\n",
    "print('shape', one_us[0].shape)\n",
    "print('min', tf.reduce_min(one_us[0]).numpy())\n",
    "print('max', tf.reduce_max(one_us[0]).numpy())\n",
    "print('norm', tf.norm(one_us[0]).numpy())\n",
    "print('max of diff', tf.reduce_max(tf.abs(usd)).numpy())\n",
    "print('norm of diff', tf.norm(usd).numpy())\n",
    "plt.matshow(one_us[0][0].numpy())\n",
    "plt.matshow(one_us[1][0].numpy())\n",
    "plt.matshow(usd[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_mod = us.numpy()\n",
    "print(us_mod.shape)\n",
    "us_mod[0][0][0][0] = np.zeros((8))\n",
    "#us_mod[0][0][0][1] = np.zeros((8))\n",
    "\n",
    "#us_mod[0][0][0][1][7] = -0.5\n",
    "\n",
    "#us_mod[0][0] = us_mod[0][0] + 2.0 * (us_mod[0][1] - us_mod[0][0])\n",
    "r = l2.reconstruct(us_mod)\n",
    "r = l1.reconstruct(r)\n",
    "y_2_mod_img = vecs_to_img(r[0][0].numpy())\n",
    "keras.utils.array_to_img(y_2_mod_img, scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1fe96",
   "metadata": {},
   "source": [
    "## Exports and animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9026ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_interpolation(x1, x2, reconstruction_fun, filename, min_coeff=0.0, max_coeff=1.0, steps=20):\n",
    "    fig = plt.figure()\n",
    "    animate = lambda i: reconstruction_fun(x1 + (min_coeff + float(i) / steps * (max_coeff - min_coeff)) * (x2 - x1))\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=steps, interval=100, blit=True)\n",
    "    anim.save(filename, fps=15, writer='imagemagick')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_l1(x):\n",
    "    recon = l1.reconstruct(x)[0][0]\n",
    "    recon_im_array = vecs_to_img(tf.squeeze(recon).numpy())\n",
    "    recon_im = keras.utils.array_to_img(recon_im_array, scale=False)\n",
    "    return plt.imshow(recon_im, cmap='gray', vmin=0, vmax=255),\n",
    "\n",
    "def reconstruct_l2(x):\n",
    "    recon = l2.reconstruct(x)\n",
    "    recon = l1.reconstruct(recon)[0][0]\n",
    "    recon_im_array = vecs_to_img(tf.squeeze(recon).numpy())\n",
    "    recon_im = keras.utils.array_to_img(recon_im_array, scale=False)\n",
    "    return plt.imshow(recon_im, cmap='gray', vmin=0, vmax=255),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "us1 = reconstruction_model.get_layer(index=1)(tf.reshape(X2[1], (1, 3, 2500, 1, 2)), training=False)[0]\n",
    "us2 = reconstruction_model.get_layer(index=1)(tf.reshape(X2[5], (1, 3, 2500, 1, 2)), training=False)[0]\n",
    "\n",
    "animate_interpolation(us1, us2, reconstruct_l1, 'l1_test.gif', min_coeff=0.0, max_coeff=1.0, steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "us1 = reconstruction_model_2.get_layer(index=1)(tf.reshape(X_2[4001], (1, 3, 1, 8, 40)), training=False)[0]\n",
    "us2 = reconstruction_model_2.get_layer(index=1)(tf.reshape(X_2[4005], (1, 3, 1, 8, 40)), training=False)[0]\n",
    "\n",
    "animate_interpolation(us1, us2, reconstruct_l2, 'l2_test.gif', min_coeff=0.0, max_coeff=1.0, steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aedccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for coordinate in range(8):\n",
    "    us1_mod1 = us.numpy()\n",
    "    us1_mod2 = us.numpy()\n",
    "    us1_mod1[0][0][0][1] = np.zeros((8))\n",
    "    us1_mod1[0][0][0][0][coordinate] = 0.0\n",
    "    us1_mod2[0][0][0][1] = np.zeros((8))\n",
    "    us1_mod2[0][0][0][0][coordinate] = 2.0\n",
    "\n",
    "    animate_interpolation(us1_mod1, us1_mod2, reconstruct_l2, f'level_2_boxworld_coordinate_{coordinate}.gif', min_coeff=-1.0, max_coeff=1.0, steps=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ddefe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
